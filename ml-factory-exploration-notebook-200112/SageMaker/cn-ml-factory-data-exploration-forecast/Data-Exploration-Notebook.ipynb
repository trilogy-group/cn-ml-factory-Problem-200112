{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify this cell\n",
    "from base64 import b64decode\n",
    "import json\n",
    "source_config = json.loads(b64decode(\"<source_config>\".encode(\"ascii\")).decode(\"ascii\"))\n",
    "metadata = json.loads(b64decode(\"<metadata>\".encode(\"ascii\")).decode(\"ascii\"))\n",
    "print(\"Source Config: {}\".format(source_config))\n",
    "print(\"Input Tables MetaData: {}\".format(metadata))\n",
    "try:\n",
    "    import pandas_profiling\n",
    "except:\n",
    "    !sudo /home/ec2-user/anaconda3/bin/conda update -n amazonei_tensorflow_p36 --all -y\n",
    "    !sudo /home/ec2-user/anaconda3/bin/conda install -c conda-forge -n amazonei_tensorflow_p36 pandas-profiling imagehash -y\n",
    "    !sudo /home/ec2-user/anaconda3/bin/conda update -n amazonei_tensorflow_p36 ipywidgets -y\n",
    "finally:\n",
    "    import pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data's metadata -  (User input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_location = source_config['input_s3_dir']\n",
    "TIMESTAMP_COLUMN_KEY = metadata['timestamp_column']\n",
    "TARGET_VALUE_COLUMN_KEY = metadata['target_column']\n",
    "ITEM_ID_COLUMN_KEY = metadata['item_id_column']\n",
    "input_forecast_dimensions_column_keys = metadata['forecast_dimension_columns']\n",
    "columns=metadata[\"columns\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "li = []\n",
    "for file in fs.ls(input_data_location):\n",
    "    print(\"reading file : \",file)\n",
    "    try:\n",
    "        li.append(pd.read_csv(\"s3://{}\".format(file)))\n",
    "    except:\n",
    "#         print('file {} is not readable'.format(file))\n",
    "        pass\n",
    "target_timeseries_df = pd.concat(li, axis=0, ignore_index=True)\n",
    "target_timeseries_df.columns=metadata['columns']\n",
    "target_timeseries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "target_timeseries_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration\n",
    "\n",
    "- Time range\n",
    "- Total datapoints\n",
    "- Unique items\n",
    "- Unique forecasts\n",
    "- Ability to plot timeseries of a particular forecast dimesion\n",
    "- Missing values for all features + target value \n",
    "- Statistics target value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_time_range(df):\n",
    "    print(\"Time range: {} to {}\".format(df[TIMESTAMP_COLUMN_KEY].min(), df[TIMESTAMP_COLUMN_KEY].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total datapoints count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_total_data_points(df):\n",
    "    print(\"Total datapoints: {}\".format(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique items count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_unique_items(df):\n",
    "    print(\"Unique items: {}\".format(len(df[\"item_id\"].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_unique_forecasts(df):\n",
    "    forecast_dimensions = df[ITEM_ID_COLUMN_KEY].map(str)\n",
    "\n",
    "    for key in input_forecast_dimensions_column_keys:\n",
    "        forecast_dimensions += '_' + df[key].map(str)\n",
    "\n",
    "    print(\"Unique forecasts: {}\".format(len(forecast_dimensions.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(df):\n",
    "    print(\"Missing values:\")\n",
    "    print(df[df.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target variable stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_variable_statistics(df):\n",
    "    print(\"Target Variable min: {}\".format(df[TARGET_VALUE_COLUMN_KEY].min()))\n",
    "    print(\"Target Variable max: {}\".format(df[TARGET_VALUE_COLUMN_KEY].max()))\n",
    "    print(\"Target Variable mean: {}\".format(df[TARGET_VALUE_COLUMN_KEY].mean()))\n",
    "    print(\"Target Variable std dev: {}\".format(df[TARGET_VALUE_COLUMN_KEY].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check sparsity of target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "target_timeseries_df[TARGET_VALUE_COLUMN_KEY].hist(bins=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot items wrt target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_time_period(df, start_time, end_time):\n",
    "    if start_time:\n",
    "        df = df[df[TIMESTAMP_COLUMN_KEY] >= start_time]\n",
    "    if end_time:\n",
    "        df = df[df[TIMESTAMP_COLUMN_KEY] <= end_time]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clip_columns(df, threshold):\n",
    "    columns = len(df.columns)\n",
    "    if columns > threshold:\n",
    "        print(\"Too many columns: {}. Truncating to {}\".format(columns, threshold))\n",
    "        df = df.iloc[:, : threshold]\n",
    "    return df\n",
    "    \n",
    "def plot_items(df, columns, max_plots=20, start_time=None, end_time=None):\n",
    "    df = clip_time_period(df, start_time, end_time)\n",
    "    \n",
    "    new_df = pd.pivot_table(df, values=TARGET_VALUE_COLUMN_KEY, index=TIMESTAMP_COLUMN_KEY, columns=columns, aggfunc=np.sum)\n",
    "    \n",
    "    new_df = clip_columns(new_df, max_plots)\n",
    "    \n",
    "    new_df.plot(subplots=True, grid=True, legend=True, figsize=(15, 1.5 * max_plots))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_explorations(df):\n",
    "    explore_time_range(df)\n",
    "    print(\"------------------------\")\n",
    "    explore_total_data_points(df)\n",
    "    print(\"------------------------\")\n",
    "    explore_unique_items(df)\n",
    "    print(\"------------------------\")\n",
    "    check_missing_values(df)\n",
    "    print(\"------------------------\")\n",
    "    explore_unique_forecasts(df)\n",
    "    print(\"------------------------\")\n",
    "    target_variable_statistics(df)\n",
    "    plot_items(df, columns=[ITEM_ID_COLUMN_KEY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "run_explorations(target_timeseries_df)\n",
    "print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exploration_profile = pandas_profiling.ProfileReport(target_timeseries_df)\n",
    "data_exploration_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you want to save the above data explorations report to your s3 bucket as html file,\n",
    "#uncomment the code below and populate the required placeholders\n",
    "\n",
    "# file_name = \"\"             #string placeholder\n",
    "# s3_bucket_path = \"\"           #string placeholder\n",
    "\n",
    "# file_name = \"{}.html\".format(file_name)\n",
    "# data_exploration_profile.to_file(output_file=file_name)\n",
    "# s3_client = boto3.client('s3')\n",
    "# response = s3_client.upload_file(file_name, s3_bucket_path, file_name\")\n",
    "# print (response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
